<!DOCTYPE html>
<html lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.101.0" />
    <meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="author" content="Diane Larlus">

<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
<link href="https://unpkg.com/tailwindcss@^1.0/dist/tailwind.min.css" rel="stylesheet">
<script src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/konpa/devicon@master/devicon.min.css">
<link rel="stylesheet" href="/css/navbar.css" />
<link rel="stylesheet" href="/css/index.css" />



<script type="text/javascript">
 window.addEventListener('scroll', ()=>{
     const nav = document.querySelector('nav')
     nav.classList.toggle('onscroll', window.scrollY > 0)
 })
</script>

    <title>Diane Larlus&#39;s Personal Page</title>
  </head>
  <body>
      <nav class="pl-4 md:pl-24">
  <a href="" class="logo">Diane Larlus&#39;s Personal Page</a>
  <ul class="hidden md:flex">
    
    <li>
      <a href="/#News">News</a>
    </li>
    
    <li>
      <a href="/#Recent%20talks">Recent talks</a>
    </li>
    
    <li>
      <a href="/#Teaching">Teaching</a>
    </li>
    
  </ul>
</nav>
<section class="banner"></section>

      <div>
    <div
        id="mainBG"
        style="background-image: url('https://picsum.photos/id/830/1920/1080');"
    >
        <div
            class="wave wave1"
            style="background-image: url('/img/wave.png');"
        ></div>
        <div
            class="wave wave2"
            style="background-image: url('/img/wave.png');"
        ></div>
        <div
            class="wave wave3"
            style="background-image: url('/img/wave.png');"
        ></div>
        <div
            class="wave wave4"
            style="background-image: url('/img/wave.png');"
        ></div>
        <div class="centerAll">
            <div class="title text-6xl font-bold text-center">
                Diane Larlus
            </div>

            <div class="text text-xl text-center">Computer Vision &amp; Machine Learning Research Scientist</div>
        </div>
    </div>
    

    <main class="container mx-auto mt-0 px-4 xl:px-0">
        <section id="about" class="md:flex justify-around">
            <div class="my-auto md:mr-4">
                <img
                    class="w-32 h-32 rounded-full"
                    src="img/diane.jpg"
                    alt="Profile"
                />
                <div class="text-3xl title-deco tracking-wide">
  About Me
</div>

                <div class="flex mt-3 gap-3 text-xl">
                    
                    <a href="mailto:diane.larlus@naverlabs.com"><i class="fas fa-envelope"></i></a>
                    <a href="https://scholar.google.com.au/citations?user=nI2oJqkAAAAJ&amp;hl=en"><i class="fas fa-graduation-cap"></i></a>
                    <a href="https://twitter.com/dlarlus"><i class="fab fa-twitter"></i></a>
                </div>
            </div>
            <div class="mt-4 md:mt-0 md:ml-0">
                <div class="text-2xl font-bold">
                    Researcher at Naver Labs Europe and MIAI Grenoble
                </div>
                <div class="text-lg mt-3">
                    <div>
                        Principal research scientist and Team Lead at Naver Labs Europe.<br />Head of the 'lifelong representation learning' chair, within the MIAI AI research institute.<br />Research on self-supervised learning, incremental and online learning, continual domain adaptation.<br />Contributions to instance-level, semantic, cross-modal, and multimodal visual retrieval.<br />Interested in neural rendering for complex scenes.<br />
                        
                    </div>
                </div>
            </div>
        </section>

        <section id="projects" class="mt-12">
            <div class="md:flex justify-center">
                <div class="text-3xl title-deco tracking-wide">
  Projects
</div>

            </div>

            
            <div
                class="mt-8 grid grid-flow-row grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4"
            >
                
                <div class="w-full rounded overflow-hidden shadow-lg">
                    <a href="https://github.com/naver/tldr" target="_blank">
                        <img
                            class="w-full h-48"
                            src="/img/oasis.png"
                            alt="Sunset in the mountains"
                        />
                    </a>
                    <div class="px-6 py-4">
                        <div class="font-bold text-xl mb-2">OASIS @CVPR22</div>
                        <p class="text-gray-700 text-base">
                                    A benchmark and a set of baselines for the realistic task of Online Adaptation for Semantic Image Segmentation. 
        CVPR 2022
    
                        </p>
                    </div>
                </div>
                
                <div class="w-full rounded overflow-hidden shadow-lg">
                    <a href="https://github.com/naver/tldr" target="_blank">
                        <img
                            class="w-full h-48"
                            src="/img/tldr.png"
                            alt="Sunset in the mountains"
                        />
                    </a>
                    <div class="px-6 py-4">
                        <div class="font-bold text-xl mb-2">TLDR @TMLR22</div>
                        <p class="text-gray-700 text-base">
                                    Unsupervised dimensionality reduction using a Barlow Twins loss. 
        TMLR 2022
    
                        </p>
                    </div>
                </div>
                
                <div class="w-full rounded overflow-hidden shadow-lg">
                    <a href="https://europe.naverlabs.com/research/computer-vision/artemis/" target="_blank">
                        <img
                            class="w-full h-48"
                            src="/img/artemis.png"
                            alt="Sunset in the mountains"
                        />
                    </a>
                    <div class="px-6 py-4">
                        <div class="font-bold text-xl mb-2">ARTEMIS @ICLR22</div>
                        <p class="text-gray-700 text-base">
                                    Garnment image retrieval, querying with an example image and a textual modifier. 
        ICLR 2022
    
                        </p>
                    </div>
                </div>
                
                <div class="w-full rounded overflow-hidden shadow-lg">
                    <a href="https://github.com/naver/FIRe" target="_blank">
                        <img
                            class="w-full h-48"
                            src="/img/super_features.gif"
                            alt="Sunset in the mountains"
                        />
                    </a>
                    <div class="px-6 py-4">
                        <div class="font-bold text-xl mb-2">SuperFeatures @ICLR22</div>
                        <p class="text-gray-700 text-base">
                                    Mid-level features for image retrieval.
        ICLR 2022
    
                        </p>
                    </div>
                </div>
                
                <div class="w-full rounded overflow-hidden shadow-lg">
                    <a href="https://www.robots.ox.ac.uk/~vadim/neuraldiff/" target="_blank">
                        <img
                            class="w-full h-48"
                            src="/img/neuraldiff.gif"
                            alt="Sunset in the mountains"
                        />
                    </a>
                    <div class="px-6 py-4">
                        <div class="font-bold text-xl mb-2">NeuralDiff @3DV21</div>
                        <p class="text-gray-700 text-base">
                                    Neural rendering as a way to segment out even seldomly moving objects in egocentric videos.
        Collaboration with VGG, Oxford. 
        3DV 2021
    
                        </p>
                    </div>
                </div>
                
                <div class="w-full rounded overflow-hidden shadow-lg">
                    <a href="https://europe.naverlabs.com/research/computer-vision/cog-benchmark/" target="_blank">
                        <img
                            class="w-full h-48"
                            src="/img/cog.png"
                            alt="Sunset in the mountains"
                        />
                    </a>
                    <div class="px-6 py-4">
                        <div class="font-bold text-xl mb-2">CoG @ICCV21</div>
                        <p class="text-gray-700 text-base">
                                    A concept generalization benchmark for visual representations, named ImageNet-CoG.
        Collaboration with Thoth, Inria. 
        ICCV 2021
    
                        </p>
                    </div>
                </div>
                
            </div>
            
            <p> <br/> <a href="/older_projects">Older projects</a>  </p>
        </section>


        
        
          
        
          
        
          
        
          <section id="News" class="mt-12">
            <div class="flex justify-center">
                <div class="text-3xl title-deco tracking-wide">
                    News
                </div>
            </div>
            <div>
                <p>2022</p>
<ul>
<li>Co-organizing the <a href="https://sslwin.org/">Self Supervised Learning: What is Next?</a> workshop at <a href="https://eccv2022.ecva.net/"><strong>ECCV22</strong></a></li>
<li>Keynote speaker at the joint <a href="https://caprfiap2022.sciencesconf.org/"><strong>CAp-RFIAP 2022</strong></a> conference</li>
<li>Paper accepted at <a href="https://eccv2022.ecva.net/"><strong>ECCV22</strong></a>: <em>Granularity-aware Adaptation for Image Retrieval over Multiple Retrieval Tasks</em></li>
<li>Manuscript accepted to the <a href="https://www.jmlr.org/tmlr/"><strong>TMLR</strong></a> journal: <em>TLDR: Twin Learning for Dimensionality Reduction</em> [<a href="https://openreview.net/forum?id=86fhqdBUbx">paper</a>,<a href="https://github.com/naver/tldr">code</a>]</li>
<li>Invited talk at the <a href="https://asu-apg.github.io/odrum/"><strong>O-DRUM</strong></a> workshop at <a href="https://cvpr2022.thecvf.com/"><strong>CVPR22</strong></a></li>
<li>Paper accepted at <a href="https://cvpr2022.thecvf.com/"><strong>CVPR22</strong></a>: <em>On the Road to Online Adaptation for Semantic Image Segmentation</em> [<a href="https://arxiv.org/abs/2203.16195">paper</a>, <a href="https://github.com/naver/oasis">code and benchmark</a>]</li>
<li>2 papers accepted at <a href="https://iclr.cc/"><strong>ICLR22</strong></a>
<ul>
<li><em>ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity</em> [<a href="https://arxiv.org/abs/2203.08101">paper</a>]</li>
<li><em>Learning Super-Features for Image Retrieval</em> [<a href="https://arxiv.org/abs/2201.13182">paper</a>,<a href="https://github.com/naver/FIRe">code</a>]</li>
</ul>
</li>
<li>Co-organizing the <a href="https://meta.wikimedia.org/wiki/Wiki-M3L"><strong>Wiki-M3L</strong></a> workshop at <a href="https://iclr.cc/"><strong>ICLR22</strong></a></li>
<li>Area Chair for <a href="http://cvpr2022.thecvf.com/"><strong>CVPR22</strong></a></li>
</ul>
<p>2021</p>
<ul>
<li>Paper accepted at <a href="https://3dv2021.surrey.ac.uk/"><strong>3DV21</strong></a>: <em>NeuralDiff: Segmenting 3D objects that move in egocentric videos</em> [<a href="https://arxiv.org/abs/2110.09936">paper</a>, <a href="https://www.robots.ox.ac.uk/~vadim/neuraldiff/">project page</a>, <a href="https://youtu.be/Ong_ITRSmns">video</a>]</li>
<li>Co-organizing the <a href="https://sites.google.com/view/imagenet-workshop/">ImageNet Past, Present, Future</a> workshop at <a href="https://nips.cc/Conferences/2021"><strong>NeurIPS21</strong></a></li>
<li>Paper accepted at <a href="http://iccv2021.thecvf.com/home"><strong>ICCV21</strong></a>: <em>Concept Greneralization in visual representations</em>  [<a href="https://europe.naverlabs.com/research/computer-vision/cog-benchmark/">project page</a>, <a href="https://www.youtube.com/watch?v=1pMkHaETA6U">video</a>, <a href="https://github.com/naver/cog">code</a>]</li>
<li>Co-organizing of the <a href="https://project.inria.fr/paiss/">PAISS Summer School 2021</a> (organized by <a href="https://miai.univ-grenoble-alpes.fr/">MIAI</a>, <a href="https://prairie-institute.fr/">PRAIRIE</a> and <a href="https://europe.naverlabs.com/">NAVER LABS Europe</a>)</li>
<li>Invited talk at the <a href="https://sites.google.com/view/wicvcvpr2021/home">Women in Computer Vision (WiCV) Workshop</a> at <a href="http://cvpr2021.thecvf.com"><strong>CVPR21</strong></a> [<a href="https://drive.google.com/file/d/1csYrvkJVNzyZqgZ9Id-K50wdLxem5NT2/view">Video</a>]</li>
<li>Invited talk at the <a href="https://aniti.univ-toulouse.fr/">ANITI</a> research institute (<a href="https://aniti.univ-toulouse.fr/2021/05/04/diane-larlus-lifelong-visual-representation-learning-from-weak-supervision-mitigating-catastrophic-forgetting">scientific seminar</a>)</li>
<li>2 papers accepted at <a href="http://cvpr2021.thecvf.com/"><strong>CVPR21</strong></a>
<ul>
<li><em>Probabilistic Embeddings for Cross-Modal Retrieval</em> [<a href="https://arxiv.org/abs/2101.05068">paper</a>, <a href="https://github.com/naver-ai/pcme">code</a>]</li>
<li><em>Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning</em> (<strong>oral</strong>) [<a href="https://arxiv.org/abs/2012.04324">paper</a>, <a href="https://europe.naverlabs.com/research/computer-vision/continual-adaptation-of-visual-representations-via-domain-randomization-and-meta-learning/">project page</a>]</li>
</ul>
</li>
<li>2 papers accepted at <a href="http://wacv2021.thecvf.com/home"><strong>WACV21</strong></a>
<ul>
<li><em>Unsupervised meta-domain adaptation for fashion retrieval</em> [<a href="https://openaccess.thecvf.com/content/WACV2021/html/Sharma_Unsupervised_Meta-Domain_Adaptation_for_Fashion_Retrieval_WACV_2021_paper.html">paper</a>, <a href="https://github.com/vivoutlaw/UDMA">code</a>, <a href="https://www.youtube.com/watch?v=3v8cBcj6KyQ">video</a>]</li>
<li><em>StacMR: Scene-Text Aware Cross-Modal Retrieval</em> [<a href="https://openaccess.thecvf.com/content/WACV2021/html/Mafla_StacMR_Scene-Text_Aware_Cross-Modal_Retrieval_WACV_2021_paper.html">paper</a>, <a href="https://github.com/AndresPMD/StacMR">code</a>, <a href="https://www.youtube.com/watch?v=gkNKt7OOtis">video</a>]</li>
</ul>
</li>
<li>Area Chair for <a href="http://cvpr2021.thecvf.com/"><strong>CVPR21</strong></a>, <a href="http://www.mva-org.jp/mva2021/"><strong>MVA21</strong></a>, <a href="http://iccv2021.thecvf.com/home"><strong>ICCV21</strong></a>, <a href="https://www.bmvc2021.com/"><strong>BMVC21</strong></a></li>
</ul>
<p>2020</p>
<ul>
<li>Best reviewer award at <a href="https://neurips.cc/Conferences/2020"><strong>NeurIPS20</strong></a></li>
<li>Keynote speaker at the <a href="https://miai.univ-grenoble-alpes.fr/actualites/miai-grenoble-alpes-organise-le-miai-day-dans-le-cadre-du-festival-transfo--842094.htm"><strong>MIAI DAY 2020</strong></a> event</li>
<li>Paper accepted at <a href="https://neurips.cc/Conferences/2020"><strong>NeurIPS20</strong></a>: <em>Hard negative mixing for contrastive learning</em> [<a href="https://europe.naverlabs.com/research/computer-vision/mochi/">project page</a>, <a href="https://vimeo.com/488154516">video</a>]</li>
<li>Invited talk at the Instance-Level Recognition <a href="https://ilr-workshop.github.io/ECCVW2020/"><strong>Workshop at ECCV20</strong></a> [<a href="https://www.youtube.com/watch?v=-wMv3HlRUDc">video</a>]</li>
<li>Paper accepted at <a href="https://eccv2020.eu/"><strong>ECCV20</strong></a>: <em>Learning Visual Representations with Caption Annotations</em> [<a href="https://europe.naverlabs.com/research/computer-vision/icmlm/">project page</a>, <a href="https://europe.naverlabs.com/blog/learning-visual-representations-with-caption-annotations-2/">blog</a>]</li>
<li>Editor for <a href="https://www.springer.com/journal/11263"><strong>International Journal of Computer Vision</strong></a></li>
<li>Industrial chair at <a href="https://eccv2020.eu/"><strong>ECCV20</strong></a></li>
<li>Area Chair for <a href="https://eccv2020.eu/"><strong>ECCV20</strong></a>,  <a href="http://cvpr2020.thecvf.com/"><strong>CVPR20</strong></a></li>
</ul>
<p>More</p>
<ul>
<li>see <a href="/older_news">Older news</a></li>
<li>see <a href="/publications">Full list of publications</a></li>
</ul>

            </div>
          </section>
          
          
        
          <section id="Recent talks" class="mt-12">
            <div class="flex justify-center">
                <div class="text-3xl title-deco tracking-wide">
                    Recent talks
                </div>
            </div>
            <div>
                <table>
<thead>
<tr>
<th style="text-align:right">  </th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right"><strong>Jul 2022</strong></td>
<td> </td>
<td>Invited talk at the <a href="http://pai.di.unipi.it/paiw2022/">1st International Workshop on Pervasive Artificial Intelligence</a> - <em>Lifelong visual representation learning</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Jul 2022</strong></td>
<td> </td>
<td>Keynote Speaker at <a href="https://caprfiap2022.sciencesconf.org/">CAp-RFIAP 2022</a> - <em>Lifelong visual representation learning</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Jun 2022</strong></td>
<td> </td>
<td>Invited talk at the <a href="https://asu-apg.github.io/odrum/">O-DRUM Workshop at CVPR22</a> - <em>Using Text in Computer Vision</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Apr 2022</strong></td>
<td> </td>
<td>Invited talk at <a href="https://gdr-tal.ls2n.fr/event/journee-apprentissage-des-representations-de-la-parole-et-du-langage/">GDR TAL Day</a> - <em>Learning transferable visual representations</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Jan 2022</strong></td>
<td> </td>
<td>Invited talk at <a href="https://ai.ku.edu.tr/all-events/">KUIS AI Meetings</a> - <em>Towards Semantic Understanding in Egocentric Videos</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Jun 2021</strong></td>
<td> </td>
<td>Invited talk at the <a href="https://sites.google.com/view/wicvcvpr2021/home">Women in Computer Vision (WiCV) Workshop</a> at CVPR21 - <em>Lifelong visual representation learning</em> [<a href="https://drive.google.com/file/d/1csYrvkJVNzyZqgZ9Id-K50wdLxem5NT2/view">Video</a>]</td>
</tr>
<tr>
<td style="text-align:right"><strong>May 2021</strong></td>
<td></td>
<td>Invited talk at the <a href="https://aniti.univ-toulouse.fr/">ANITI</a> research institute (<a href="https://aniti.univ-toulouse.fr/2021/05/04/diane-larlus-lifelong-visual-representation-learning-from-weak-supervision-mitigating-catastrophic-forgetting">scientific seminar</a>) - <em>Learning from weak supervision and mitigating catastrophic forgetting</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Oct 2020</strong></td>
<td></td>
<td>Keynote Speaker for <a href="https://miai.univ-grenoble-alpes.fr/events-highlights/miai-days/miai-grenoble-alpes-organises-the-miai-day-within-the-framework-of-the-festival-tansfo-2020-845085.htm?RH=1617121028074">MIAI Days</a> - <em>Lifelong visual representation learning</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Aug 2020</strong></td>
<td></td>
<td>Invited talk at the <a href="https://ilr-workshop.github.io/ECCVW2020/">ILR Workshop at ECCV20</a>  - <em>From Instance-Level to Semantic Image Retrieval</em> [<a href="https://www.youtube.com/watch?v=-wMv3HlRUDc&amp;t=1s">Video</a>]</td>
</tr>
<tr>
<td style="text-align:right"><strong>Nov 2019</strong></td>
<td></td>
<td>Invited talk at the <a href="http://thoth.inrialpes.fr/people/mairal/solaris/workshop2019/">SOLARIS Workshop</a> -  <em>Self-supervised learning for category-level geometry estimation</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Apr 2019</strong></td>
<td></td>
<td>Invited talk at the <a href="https://www.sfds.asso.fr/">StatLearn</a> 19 conference - <em>Learning Image Representations for Efficient Visual Search</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Nov 2018</strong></td>
<td></td>
<td>Invited speaker, AI and Vision session at <a href="https://univ-cotedazur.fr/events-uca/sophia-summit">SophIA Summit</a> - <em>Visual Search</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Jul 2018</strong></td>
<td></td>
<td>Lecture at the PRAIRIE Artificial Intelligence summer school (<a href="https://project.inria.fr/paiss/home-2018/">PAISS</a>) [<a href="thoth.inrialpes.fr/workshop/paiss2018/larlus-visual-search.pdf">slides</a>] - <em>Visual Search</em></td>
</tr>
<tr>
<td style="text-align:right"></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align:right"></td>
<td></td>
<td><strong>Older</strong>: see CV</td>
</tr>
</tbody>
</table>

            </div>
          </section>
          
          
        
          <section id="Teaching" class="mt-12">
            <div class="flex justify-center">
                <div class="text-3xl title-deco tracking-wide">
                    Teaching
                </div>
            </div>
            <div>
                <table>
<thead>
<tr>
<th style="text-align:right">  </th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right"><strong>2022</strong></td>
<td></td>
<td>Découvrir l’IA par le jeu (<em>Discover AI with games</em>, a 2 day training session for high school teachers) - <em>Maison pour la Science Alpes-Dauphiné</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Jan 2022</strong></td>
<td> </td>
<td>Guest lecture on Visual Search  - <em>Posts and Telecommunications Institute of Technology (PTIT), Vietnam</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>2021/2022</strong></td>
<td> </td>
<td>Comprendre des données visuelles à grande échelle - <em>ENSIMAG, Grenoble</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>May 2021</strong></td>
<td> </td>
<td>Guest lecture on Visual Search  - <em>Hanoi University of Science and Technology (HUST), Vietnam</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>2020/2021</strong></td>
<td> </td>
<td>Comprendre des données visuelles à grande échelle - <em>ENSIMAG, Grenoble</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>2020</strong></td>
<td></td>
<td>Découvrir l’IA par le jeu (<em>Discover AI with games</em>, a 2 day training session for high school teachers) - <em>Maison pour la Science Alpes-Dauphiné</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Nov 2019</strong></td>
<td> </td>
<td>Lecture on Visual Search as part of the &lsquo;Deep Learning&rsquo; course for the MVA master - <em>ENS Paris</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>2019/2020</strong></td>
<td> </td>
<td>Comprendre des données visuelles à grande échelle - <em>ENSIMAG, Grenoble</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>Feb 2019</strong></td>
<td></td>
<td>Lecture on Visual Search as part of the &lsquo;Deep Learning in Practice&rsquo; course for the MVA master - <em>ENS Paris</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>2018/2019</strong></td>
<td></td>
<td>Comprendre des données visuelles à grande échelle - <em>ENSIMAG, Grenoble</em></td>
</tr>
<tr>
<td style="text-align:right"><strong>July 2018</strong></td>
<td></td>
<td>Lecture on Visual Search at the <em>PRAIRIE Artificial Intelligence summer school (<a href="https://project.inria.fr/paiss/program-2018/">PAISS 2018</a>)</em>  [<a href="thoth.inrialpes.fr/workshop/paiss2018/larlus-visual-search.pdf">slides</a>]</td>
</tr>
<tr>
<td style="text-align:right"></td>
<td></td>
<td><strong>Older</strong>: see CV</td>
</tr>
</tbody>
</table>

            </div>
          </section>
          
          
    </main>
</div>


      <footer class="text-center mt-8 py-2 bg-gray-900 text-white">
  <div>&copy; 2022 Diane Larlus - Hugo (SimpleIntro)</div>
</footer>

  </body>
</html>
