<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Diane Larlus&#39;s Personal Page</title>
    <link>/</link>
    <description>Recent content on Diane Larlus&#39;s Personal Page</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 10 Jan 2021 18:48:42 +0100</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>News</title>
      <link>/news/</link>
      <pubDate>Sun, 10 Jan 2021 18:48:42 +0100</pubDate>
      <guid>/news/</guid>
      <description>&lt;p&gt;2025&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;2 papers accepted at &lt;a href=&#34;https://cvpr2025.thecvf.com&#34;&gt;&lt;strong&gt;CVPR25&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;DUNE: Distilling a Universal Encoder from Heterogenous 2D and 3D Teachers&lt;/em&gt; [&lt;a href=&#34;https://europe.naverlabs.com/research/publications/dune/&#34;&gt;Project page&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Layered motion fusion: Lifting motion segmentation to 3D in egocentric videos&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Invited as a lecturer for the &lt;a href=&#34;https://ivi.fnwi.uva.nl/ellis/events/2025-ellis-winter-school-on-foundation-models-fomo/&#34;&gt;2nd ELLIS Winter School on Foundation Models (FoMo25)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Keynote speaker at the &lt;a href=&#34;https://visigrapp.scitevents.org/&#34;&gt;&lt;strong&gt;VISIGRAPP 2025 conference&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Area Chair for &lt;a href=&#34;https://iccv.thecvf.com/&#34;&gt;&lt;strong&gt;ICCV25&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;2024&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Invited speaker at two &lt;a href=&#34;https://eccv.ecva.net&#34;&gt;&lt;strong&gt;ECCV 2024&lt;/strong&gt;&lt;/a&gt; workshops: the &lt;a href=&#34;https://sites.google.com/view/cv4metaverse-2024/&#34;&gt;Computer Vision for Metaverse&lt;/a&gt; and the &lt;a href=&#34;https://sites.google.com/view/eccv24-tricky-workshop/home&#34;&gt;Transparent &amp;amp; Reflective objects In the wild (TRICKY)&lt;/a&gt; ones&lt;/li&gt;&#xA;&lt;li&gt;Paper accepted at &lt;a href=&#34;https://eccv.ecva.net/&#34;&gt;&lt;strong&gt;ECCV 2024&lt;/strong&gt;&lt;/a&gt;: &lt;em&gt;UNIC: Universal Classification Models via Multi-teacher Distillation&lt;/em&gt; [&lt;a href=&#34;https://europe.naverlabs.com/unic&#34;&gt;Project page&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;li&gt;Paper accepted at &lt;a href=&#34;https://lifelong-ml.cc/&#34;&gt;&lt;strong&gt;CoLLAs 2024&lt;/strong&gt;&lt;/a&gt;: &lt;em&gt;PANDAS: Prototype-based Novel Class Discovery and Detection&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/pdf/2402.17420&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/naver/pandas&#34;&gt;code&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;li&gt;Invited for a keynote at the &lt;a href=&#34;https://lifelong-ml.cc/&#34;&gt;&lt;strong&gt;CoLLAs 2024&lt;/strong&gt;&lt;/a&gt; conference&lt;/li&gt;&#xA;&lt;li&gt;Paper accepted at &lt;a href=&#34;https://jmlr.org/tmlr/&#34;&gt;&lt;strong&gt;TMLR&lt;/strong&gt;&lt;/a&gt;: &lt;em&gt;On Good Practices for Task-Specific Distillation of Large Pretrained Visual Models&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2402.11305&#34;&gt;Paper&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;li&gt;Invited as a lecturer for the International Computer Vision Summer School &lt;a href=&#34;https://iplab.dmi.unict.it/icvss2024/&#34;&gt;&lt;strong&gt;ICVSS 2024&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Invited speaker at the &lt;a href=&#34;https://egovis.github.io/cvpr24/&#34;&gt;First Joint Egocentric Vision (EgoVis)&lt;/a&gt; workshop at &lt;a href=&#34;https://cvpr.thecvf.com/Conferences/2024&#34;&gt;&lt;strong&gt;CVPR24&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Paper accepted at &lt;a href=&#34;https://iclr.cc/&#34;&gt;&lt;strong&gt;ICLR24&lt;/strong&gt;&lt;/a&gt;: &lt;em&gt;Weatherproofing Retrieval for Localization with Generative AI &amp;amp; Geometric Consistency&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2402.09237&#34;&gt;Paper&lt;/a&gt;, &lt;a href=&#34;https://europe.naverlabs.com/research/computer-vision/weatherproofing-retrieval-for-localization-with-generative-ai-and-geometric-consistency/&#34;&gt;project page&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;li&gt;Area Chair for &lt;a href=&#34;https://cvpr.thecvf.com/&#34;&gt;&lt;strong&gt;CVPR24&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://eccv2024.ecva.net/&#34;&gt;&lt;strong&gt;ECCV24&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;2023&lt;/p&gt;</description>
    </item>
    <item>
      <title>Publications</title>
      <link>/publications/</link>
      <pubDate>Sun, 10 Jan 2021 18:48:42 +0100</pubDate>
      <guid>/publications/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;ICLR24&lt;/strong&gt; - &lt;em&gt;Weatherproofing Retrieval for Localization with Generative AI &amp;amp; Geometric Consistency&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Yannis Kalantidis, Mert Bülent Sarıyıldız, Rafael S. Rezende, Philippe Weinzaepfel, Diane Larlus, Gabriela Csurka&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://europe.naverlabs.com/ret4loc&#34;&gt;project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;NeurIPS23&lt;/strong&gt; - &lt;em&gt;EPIC Fields: Marrying 3D Geometry and Video Understanding&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Vadim Tschernezki, Ahmad Darkhalil, Zhifan Zhu, David Fouhey, Iro Laina, Diane Larlus, Dima Damen, Andrea Vedaldi&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://epic-kitchens.github.io/epic-fields/&#34;&gt;project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CoLLAs23&lt;/strong&gt; - &lt;em&gt;RaSP: Relation-aware Semantic Prior for Weakly Supervised Incremental Segmentation&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Subhankar Roy, Riccardo Volpi, Gabriela Csurka, Diane Larlus&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://arxiv.org/abs/2305.19879&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CVPR23&lt;/strong&gt; - &lt;em&gt;Fake it till you make it: learning transferable representations from synthetic ImageNet clones&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, Yannis Kalantidis&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://arxiv.org/abs/2212.08420&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CVPR23&lt;/strong&gt; - &lt;em&gt;SLACK: Stable Learning of Augmentations with Cold-start and KL regularization&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Juliette Marrie. Michael Arbel, Diane Larlus, Julien Mairal.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://arxiv.org/abs/2306.09998&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ICLR23&lt;/strong&gt; - &lt;em&gt;No Reason for No Supervision: Improved Generalization in Supervised Models&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Mert Bulent Sariyildiz, Yannis Kalantidis, Karteek Alahari, Diane Larlus&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://arxiv.org/abs/2206.15369&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ECCV22&lt;/strong&gt; - &lt;em&gt;Granularity-aware Adaptation for Image Retrieval over Multiple Retrieval Tasks&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Jon Almazan, Byungsoo Ko, Geonmo Gu, Diane Larlus, Yannis Kalantidis&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://arxiv.org/abs/2210.02254&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;3DV22&lt;/strong&gt; - &lt;em&gt;Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Vadim Tschernezki, Iro Laina, Diane Larlus, Andrea Vedaldi&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://www.robots.ox.ac.uk/~vadim/n3f/&#34;&gt;project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CVPR22&lt;/strong&gt; - &lt;em&gt;On the Road to Online Adaptation for Semantic Image Segmentation&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Riccardo Volpi, Pau de Jorge, Diane Larlus, Gabriela Csurka&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;hhttps://europe.naverlabs.com/research/computer-vision/oasis/&#34;&gt;project page&lt;/a&gt; &amp;ndash; &lt;a href=&#34;https://github.com/naver/oasis&#34;&gt;code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;TMLR22&lt;/strong&gt; - &lt;em&gt;TLDR: Twin Learning for Dimensionality Reduction&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Yannis Kalantidis, Jon Alamzan, Carlos Lassance, Diane Larlus, Yannis Kalantidis&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://github.com/naver/tldr&#34;&gt;code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ICLR22&lt;/strong&gt; - *ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity&lt;/li&gt;&#xA;&lt;li&gt;Ginger Delmas, Rafael Sampaio de Rezende, Gabriela Csurka, Diane Larlus&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://github.com/naver/artemis&#34;&gt;code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ICLR22&lt;/strong&gt; - &lt;em&gt;Learning Super-Features for Image Retrieval&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Philippe Weinzaepfel, Thomas Lucas, Diane Larlus, Yannis Kalantidis&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://github.com/naver/artemis&#34;&gt;code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ICCV21&lt;/strong&gt; - &lt;em&gt;Concept Generalization in Visual Representation Learning&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Mert Bulent Sariyildiz, Yannis Kalantidis, Diane Larlus, Karteek Alahari&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://europe.naverlabs.com/research/computer-vision/cog-benchmark/&#34;&gt;project page&lt;/a&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=FLSlRQBFow0&amp;amp;t=1s&#34;&gt;video&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;3DV21&lt;/strong&gt;  - &lt;em&gt;NeuralDiff: Segmenting 3D objects that move in egocentric videos&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Vadim Tschernezki, Diane Larlus, Andrea Vedaldi&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://www.robots.ox.ac.uk/~vadim/neuraldiff&#34;&gt;project page&lt;/a&gt; - &lt;a href=&#34;https://github.com/dichotomies/NeuralDiff&#34;&gt;code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CVPR21&lt;/strong&gt;  - &lt;em&gt;Probabilistic Embeddings for Cross-Modal Retrieval&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio de Rezende, Yannis Kalantidis, Diane Larlus&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://arxiv.org/abs/2101.05068&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/naver-ai/pcme&#34;&gt;code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;WACV21&lt;/strong&gt; - &lt;em&gt;Unsupervised Meta-Domain Adaptation for Fashion Retrieval&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Vivek Sharma, Naila Murray, Diane Larlus, Saquib Sarfraz, Rainer Stiefelhagen, Gabriela Csurka&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://openaccess.thecvf.com/content/WACV2021/papers/Sharma_Unsupervised_Meta-Domain_Adaptation_for_Fashion_Retrieval_WACV_2021_paper.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;WACV21&lt;/strong&gt; - &lt;em&gt;StacMR: Scene-Text Aware Cross-Modal Retrieval&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Andres Mafla, Rafael Sampaio de Rezende, Lluis Gomez, Diane Larlus, Dimosthenis Karatzas&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://europe.naverlabs.com/research/computer-vision/stacmr-scene-text-aware-cross-modal-retrieval/&#34;&gt;project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;NeurIPS20&lt;/strong&gt; - &lt;em&gt;Hard negative mixing for contrastive learning&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Yannis Kalantidis, Mert Bülent Sariyildiz, Noé Pion, Philippe Weinzaepfel, Diane Larlus.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://europe.naverlabs.com/research/computer-vision/mochi/&#34;&gt;project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ECCV20&lt;/strong&gt; - &lt;em&gt;Learning Visual Representations with Caption Annotations&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Mert Bülent Sariyildiz, Julien Perez, Diane Larlus&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://europe.naverlabs.com/research/computer-vision/icmlm/&#34;&gt;project page&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ICCV19&lt;/strong&gt; - &lt;em&gt;Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Michael Wray, Diane Larlus, Gabriela Csurka, Dima Damen&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://mwray.github.io/FGAR/&#34;&gt;project page&lt;/a&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=FLSlRQBFow0&amp;amp;t=1s&#34;&gt;video&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;PAMI18&lt;/strong&gt; - &lt;em&gt;Capturing the geometry of object categories from video supervision&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;David Novotny, Diane Larlus, Andrea Vedaldi&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://ora.ox.ac.uk/objects/uuid:51fa438e-ed36-4043-a6f4-a30609c9e428&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ECCV18&lt;/strong&gt; - &lt;em&gt;Semi-convolutional Operators for Instance Segmentation&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;David Novotny, Samuel Albanie, Diane Larlus, Andrea Vedaldi&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2018/Novotny18b/novotny2018b.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CVPR18&lt;/strong&gt; - &lt;em&gt;Self-supervised learning of geometrically stable features through probabilistic introspection&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;David Novotny, Samuel Albanie, Diane Larlus, Andrea Vedaldi.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/research/probabilistic_introspection/&#34;&gt;project page&lt;/a&gt; - &lt;a href=&#34;https://youtu.be/Jl1NeziAHFY?t=5237&#34;&gt;video&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ICCV17&lt;/strong&gt; - &lt;em&gt;Learning 3D Object Categories by Looking Around Them&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;David Novotny, Diane Larlus, Andrea Vedaldi&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://arxiv.org/abs/1705.03951&#34;&gt;paper&lt;/a&gt; - &lt;a href=&#34;https://www.youtube.com/watch?v=esYBbQuKFZU&#34;&gt;video&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;IJCV17&lt;/strong&gt; - &lt;em&gt;End-to-end learning of deep visual representations for image retrieval&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Albert Gordo, Jon Almaz&amp;rsquo;an, Jérôme Revaud, Diane Larlus.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://arxiv.org/abs/1610.07940&#34;&gt;paper&lt;/a&gt; - &lt;a href=&#34;https://github.com/naver/deep-image-retrieval&#34;&gt;code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CVPR17&lt;/strong&gt; - &lt;em&gt;Beyond instance-level image retrieval: Leveraging captions to learn a global visual representation for semantic retrieval&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Albert Gordo, Diane Larlus&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2017/papers/Gordo_Beyond_Instance-Level_Image_CVPR_2017_paper.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CVPR17&lt;/strong&gt; - &lt;em&gt;AnchorNet: A Weakly Supervised Network to Learn Geometry-sensitive Features For Semantic Matching&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;David Novotny, Diane Larlus, Andrea Vedaldi.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://d-novotny.github.io/papers/novotny17anchornet.pdf&#34;&gt;paper&lt;/a&gt; - &lt;a href=&#34;https://d-novotny.github.io/code/anchornet_code_v0.1.zip&#34;&gt;code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Book Chapter 2017&lt;/strong&gt; - &lt;em&gt;Generalizing Semantic Part Detectors Across Domains&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;David Novotny, Diane Larlus, Andrea Vedaldi&lt;/li&gt;&#xA;&lt;li&gt;in &lt;em&gt;Domain Adaptation in Computer Vision Applications&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ECCV16&lt;/strong&gt; - &lt;em&gt;Deep image retrieval: Learning global representations for image search&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Albert Gordo, Jon Almazán, Jérôme Revaud, Diane Larlus&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://arxiv.org/abs/1604.01325&#34;&gt;paper&lt;/a&gt; - &lt;a href=&#34;https://github.com/naver/deep-image-retrieval&#34;&gt;code&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ECCV Workshop 2016&lt;/strong&gt; - &lt;em&gt;Learning the structure of objects from web supervision&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;David Novotny, Diane Larlus, Andrea Vedaldi&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://d-novotny.github.io/papers/novotny16learning.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;BMVC16&lt;/strong&gt; - &lt;em&gt;I Have Seen Enough: Transferring Parts Across Categories&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;David Novotny, Diane Larlus, Andrea Vedaldi&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://d-novotny.github.io/papers/novotny16ihave.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;TPAMI 2015&lt;/strong&gt; - &lt;em&gt;Data-driven Detection of Prominent Objects&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Jose Rodriguez-Serrano, Diane Larlus, Zhenwen Dai&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CVPR15&lt;/strong&gt; - &lt;em&gt;Fisher vectors meet neural networks: A hybrid classification architecture&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Florent Perronnin, Diane Larlus&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2015/papers/Perronnin_Fisher_Vectors_Meet_2015_CVPR_paper.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ICCV13&lt;/strong&gt; - &lt;em&gt;Predicting an Object Location Using a Global Image Representation&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Jose Rodriguez Serrano, Diane Larlus&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://openaccess.thecvf.com/content_iccv_2013/papers/Serrano_Predicting_an_Object_2013_ICCV_paper.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;BMVC13&lt;/strong&gt; - &lt;em&gt;What is a good evaluation measure for semantic segmentation?&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Gabriela Csurka, Diane Larlus, Florent Perronnin.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;http://www.bmva.org/bmvc/2013/Papers/paper0032/paper0032.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ICVGIP12&lt;/strong&gt; - &lt;em&gt;On the use of regions for semantic image segmentation&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Rui Hu, Diane Larlus, Gabriela Csurka.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ECCV Workshop 2012&lt;/strong&gt; - &lt;em&gt;Semantic image segmentation using visible and near-infrared channels&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;Neda Salamati, Diane Larlus, Gabriela Csurka, Sabine Süsstrunk&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Book Chapter 2012&lt;/strong&gt; - &lt;em&gt;Color naming&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;WR Benavente, M Vanrell, C Schmid, R Baldrich, J Verbeek, D Larlus&lt;/li&gt;&#xA;&lt;li&gt;in &lt;em&gt;Color in Computer Vision: Fundamentals and Applications&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;http://cat.cvc.uab.es/~joost/papers/NamingTIP09.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ICCV2021&lt;/strong&gt; - &lt;em&gt;Assessing the aesthetic quality of photographs using generic image descriptors&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;L L Marchesotti, F Perronnin, D Larlus, G Csurka&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://web.archive.org/web/20131126045505id_/http://www.tamaraberg.com:80/teaching/Fall_13/papers/Marchesotti2011.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;TPAMI 2011&lt;/strong&gt; - &lt;em&gt;Weakly supervised recognition of daily life activities with wearable sensors&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;M Stikic, D Larlus, S Ebert, B Schiele&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;BMVC11&lt;/strong&gt; - &lt;em&gt;Combining Visible and Near-Infrared Cues for Image Categorisation&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;N Salamati, D Larlus, G Csurka&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;http://www.bmva.org/bmvc/2011/proceedings/paper49/paper49.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Journal of Signal Processing Systems 2010&lt;/strong&gt; - &lt;em&gt;Manifold based local classifiers: Linear and nonlinear approaches&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;H Cevikalp, D Larlus, M Neamtu, B Triggs, F Jurie&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://hal.science/hal-00565007/file/Cevikalp-jsps10.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ECCV10&lt;/strong&gt; - &lt;em&gt;Extracting structures in image collections for object recognition&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;S Ebert, D Larlus, B Schiele.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://www.mpi-inf.mpg.de/fileadmin/inf/d2/Research_projects_files/EbertECCV2010.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;IJCV10&lt;/strong&gt; - &lt;em&gt;Category level object segmentation by combining bag-of-words models with dirichlet processes and random fields&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;D Larlus, J Verbeek, F Jurie.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://inria.hal.science/inria-00439303v2/document&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Pervasive Workshop 2010&lt;/strong&gt; - &lt;em&gt;Standing on the Shoulders of Other Researchers A Position Statement&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;U Blanke, D Larlus, K Van Laerhoven, B Schiele&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;WACV09&lt;/strong&gt; - &lt;em&gt;Multi-graph based semi-supervised learning for activity recognition&lt;/em&gt; -  &lt;strong&gt;(Best paper award)&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;M Stikic, D Larlus, B Schiele&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://www.academia.edu/download/48670601/Multi-graph_Based_Semi-supervised_Learni20160908-18937-18e1as5.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;TIP09&lt;/strong&gt; - &lt;em&gt;Learning color names for real-world applications&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;J Van De Weijer, C Schmid, J Verbeek, D Larlus&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://hal.inria.fr/inria-00439284v2/document&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Image and Vision Computing 2009&lt;/strong&gt; - &lt;em&gt;Latent mixture vocabularies for object categorization and segmentation&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;D Larlus, F Jurie&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://jurie.users.greyc.fr/papers/09-imavis-larlus-jurie.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Humanoids 2008&lt;/strong&gt; - &lt;em&gt;Treasure hunting for humanoids robot&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;O Stasse, T Foissotte, D Larlus, A Kheddar, K Yokoi&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://homepages.laas.fr/ostasse/os_cognitivevision.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;PhD Thesis 2008&lt;/strong&gt; - &lt;em&gt;Création et utilisation de vocabulaires visuels pour la catégorisation d&amp;rsquo;images et la segmentation de classes d&amp;rsquo;objets&lt;/em&gt;. D Larlus.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://tel.archives-ouvertes.fr/tel-00343665&#34;&gt;manuscript&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CVPR08&lt;/strong&gt; - &lt;em&gt;Combining appearance models and markov random fields for category level object segmentation&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;D Larlus, F Jurie&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://inria.hal.science/inria-00548660v1/document&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Humanoids 2007&lt;/strong&gt; - &lt;em&gt;Towards autonomous object reconstruction for visual search by the humanoid robot hrp-2&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;O Stasse, D Larlus, B Lagarde, A Escande, F Saidi, A Kheddar, K Yokoi, F Jurie&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://inria.hal.science/inria-00548676/document&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;SPCA 2007&lt;/strong&gt; - &lt;em&gt;A supervised clustering algorithm for the initialization of rbf neural network classifiers&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;H Cevikalp, D Larlus, F Jurie&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/4298803&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;VISAPP 2007&lt;/strong&gt; - &lt;em&gt;Category level object segmentation&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;D Larlus, F Jurie&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://jurie.users.greyc.fr/papers/07-visapp.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Workshop on Machine Learning for Signal Processing 2007&lt;/strong&gt; - &lt;em&gt;Local subspace Classifiers: Linear and Non Linear Approaches&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;H Cevikalp, D Larlus, M Douze, F Jurie&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-00203992/document&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;BMVC06&lt;/strong&gt; - &lt;em&gt;Latent mixture vocabularies for object categorization&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;D Larlus, F Jurie.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-00203721&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ECCV Workshop 2006&lt;/strong&gt; - &lt;em&gt;Learning saliency maps for object categorization&lt;/em&gt;&lt;/li&gt;&#xA;&lt;li&gt;F Moosmann, D Larlus, F Jurie.&lt;/li&gt;&#xA;&lt;li&gt;└─ &lt;a href=&#34;https://jurie.users.greyc.fr/papers/MLJ06.pdf&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;hr&gt;&#xA;&lt;hr&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Back to the &lt;a href=&#34;/index.html&#34;&gt;Main Page&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Recent talks</title>
      <link>/talks/</link>
      <pubDate>Sun, 10 Jan 2021 18:48:42 +0100</pubDate>
      <guid>/talks/</guid>
      <description>&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: right&#34;&gt;  &lt;/th&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Mar 2025&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Lecture at the &lt;a href=&#34;https://sites.google.com/view/eccv24-tricky-workshop/home&#34;&gt;ELLIS Winter School on Foundation Models (FoMo)&lt;/a&gt; - &lt;em&gt;Lifelong learning in the Age of Foundation Models&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Fev 2025&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Keynote Speaker at the &lt;a href=&#34;https://visigrapp.scitevents.org/&#34;&gt;Joint VISIGRAPP Conference&lt;/a&gt; - &lt;em&gt;Lifelong Visual Representation Learning&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Oct 2024&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://sites.google.com/view/eccv24-tricky-workshop/home&#34;&gt;Transparent &amp;amp; Reflective objects In the wild (TRICKY) workshop at ECCV24&lt;/a&gt; - &lt;em&gt;Towards a Geometric Understanding in Egocentric Videos&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Oct 2024&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://sites.google.com/view/cv4metaverse-2024/&#34;&gt;Computer Vision for Metaverse workshop at ECCV24&lt;/a&gt; - &lt;em&gt;Towards a Geometric Understanding in Egocentric Videos&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Aug 2024&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Keynote Speaker at the &lt;a href=&#34;https://lifelong-ml.cc/Conferences/2024/speakers&#34;&gt;Third Conference on Lifelong Learning Agents (CoLLAs) 2024&lt;/a&gt; - &lt;em&gt;Lifelong learning for visual representations&lt;/em&gt; [&lt;a href=&#34;https://youtu.be/_KZhNcLxuvY?si=tSK9jyz2RmxWZu1w&#34;&gt;Video&lt;/a&gt;]&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jul 2024&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Lecture at the &lt;a href=&#34;https://iplab.dmi.unict.it/icvss2024/&#34;&gt;International Computer Vision Summer School (ICVSS) 2024&lt;/a&gt; - &lt;em&gt;Lifelong learning for visual representations&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jun 2024&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://egovis.github.io/cvpr24/&#34;&gt;First Joint Egocentric Vision (EgoVis) at CVPR24&lt;/a&gt; - &lt;em&gt;Towards a Geometric Understanding in Egocentric Videos&lt;/em&gt; [&lt;a href=&#34;https://youtu.be/b_M_QNKAhTQ?si=WhpjM2jakPYPXYjw&amp;amp;t=8895&#34;&gt;Video&lt;/a&gt;]&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Dec 2023&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://sslneurips23.github.io&#34;&gt; Self-Supervised Learning - Theory and Practice workshop at NeurIPS23&lt;/a&gt; - &lt;em&gt;Learning visual representations that transfers&lt;/em&gt;  [&lt;a href=&#34;https://neurips.cc/virtual/2023/84303&#34;&gt;Video&lt;/a&gt;]&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Oct 2023&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://iplab.dmi.unict.it/acvr2023/&#34;&gt;Assistive Computer Vision and Robotics workshop at ICCV23&lt;/a&gt; - &lt;em&gt;Towards a Geometric Understanding in Egocentric Videos&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jun 2023&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://sites.google.com/view/clvision2023&#34;&gt;Continual Learning in Computer Vision workshop at CVPR23&lt;/a&gt; - &lt;em&gt;Lifelong visual representation learning&lt;/em&gt; [&lt;a href=&#34;https://youtu.be/uiNMnyoUiso?si=X7VFjm4JNZtFgXLq&amp;amp;t=2792&#34;&gt;Video&lt;/a&gt;]&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jun 2023&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://www.grss-ieee.org/events/earthvision-2023/&#34;&gt;EarthVision workshop at CVPR23&lt;/a&gt; - &lt;em&gt;Lifelong visual representation learning&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Sep 2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;strong&gt;Thoth team&lt;/strong&gt; at Inria - &lt;em&gt;Towards Semantic Understanding in Egocentric Videos&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Sep 2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;strong&gt;Linkmedia Speaks Science&lt;/strong&gt; seminar series - &lt;em&gt;Lifelong visual representation learning&lt;/em&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=Jnn9Vz8INKY&amp;amp;t=6s&#34;&gt;Video&lt;/a&gt;]&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jul 2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;http://pai.di.unipi.it/paiw2022/&#34;&gt;1st International Workshop on Pervasive Artificial Intelligence&lt;/a&gt; - &lt;em&gt;Lifelong visual representation learning&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jul 2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Keynote Speaker at &lt;a href=&#34;https://caprfiap2022.sciencesconf.org/&#34;&gt;CAp-RFIAP 2022&lt;/a&gt; - &lt;em&gt;Lifelong visual representation learning&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jun 2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://asu-apg.github.io/odrum/&#34;&gt;O-DRUM Workshop at CVPR22&lt;/a&gt; - &lt;em&gt;Using Text in Computer Vision&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Apr 2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at &lt;a href=&#34;https://gdr-tal.ls2n.fr/event/journee-apprentissage-des-representations-de-la-parole-et-du-langage/&#34;&gt;GDR TAL Day&lt;/a&gt; - &lt;em&gt;Learning transferable visual representations&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jan 2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at &lt;a href=&#34;https://ai.ku.edu.tr/all-events/&#34;&gt;KUIS AI Meetings&lt;/a&gt; - &lt;em&gt;Towards Semantic Understanding in Egocentric Videos&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jun 2021&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt; &lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://sites.google.com/view/wicvcvpr2021/home&#34;&gt;Women in Computer Vision (WiCV) Workshop&lt;/a&gt; at CVPR21 - &lt;em&gt;Lifelong visual representation learning&lt;/em&gt; [&lt;a href=&#34;https://drive.google.com/file/d/1csYrvkJVNzyZqgZ9Id-K50wdLxem5NT2/view&#34;&gt;Video&lt;/a&gt;]&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;May 2021&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://aniti.univ-toulouse.fr/&#34;&gt;ANITI&lt;/a&gt; research institute (&lt;a href=&#34;https://aniti.univ-toulouse.fr/2021/05/04/diane-larlus-lifelong-visual-representation-learning-from-weak-supervision-mitigating-catastrophic-forgetting&#34;&gt;scientific seminar&lt;/a&gt;) - &lt;em&gt;Learning from weak supervision and mitigating catastrophic forgetting&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Oct 2020&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Keynote Speaker for &lt;a href=&#34;https://miai.univ-grenoble-alpes.fr/events-highlights/miai-days/miai-grenoble-alpes-organises-the-miai-day-within-the-framework-of-the-festival-tansfo-2020-845085.htm?RH=1617121028074&#34;&gt;MIAI Days&lt;/a&gt; - &lt;em&gt;Lifelong visual representation learning&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Aug 2020&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://ilr-workshop.github.io/ECCVW2020/&#34;&gt;ILR Workshop at ECCV20&lt;/a&gt;  - &lt;em&gt;From Instance-Level to Semantic Image Retrieval&lt;/em&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=-wMv3HlRUDc&amp;amp;t=1s&#34;&gt;Video&lt;/a&gt;]&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Nov 2019&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;http://thoth.inrialpes.fr/people/mairal/solaris/workshop2019/&#34;&gt;SOLARIS Workshop&lt;/a&gt; -  &lt;em&gt;Self-supervised learning for category-level geometry estimation&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Apr 2019&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Invited talk at the &lt;a href=&#34;https://www.sfds.asso.fr/&#34;&gt;StatLearn&lt;/a&gt; 19 conference - &lt;em&gt;Learning Image Representations for Efficient Visual Search&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Nov 2018&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Invited speaker, AI and Vision session at &lt;a href=&#34;https://univ-cotedazur.fr/events-uca/sophia-summit&#34;&gt;SophIA Summit&lt;/a&gt; - &lt;em&gt;Visual Search&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jul 2018&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Lecture at the &lt;a href=&#34;https://project.inria.fr/paiss/program-2018/&#34;&gt;PRAIRIE Artificial Intelligence summer school (PAISS 2018)&lt;/a&gt; - &lt;em&gt;Visual Search&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Older&lt;/strong&gt;: see CV&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>Teaching</title>
      <link>/teaching/</link>
      <pubDate>Sun, 10 Jan 2021 18:48:42 +0100</pubDate>
      <guid>/teaching/</guid>
      <description>&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: right&#34;&gt;  &lt;/th&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Mar 2025&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Speaker at the &lt;em&gt;ELLIS Winter School on Foundation Models, Amsterdam&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2024/2025&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Perception, Vision et Apprentissage (Partie apprentissage continu de représentations visuelles) - master level - &lt;em&gt;ENSIMAG, Grenoble&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Aug 2024&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Lecture on Continual Learning at the &lt;em&gt;International Computer Vision Summer School, Sicily&lt;/em&gt; (&lt;a href=&#34;https://iplab.dmi.unict.it/icvss2024/&#34;&gt;ICVSS 2024&lt;/a&gt;)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2024&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Découvrir l’IA par le jeu (&lt;em&gt;Discover AI with games&lt;/em&gt;, a 2 day training session for high school teachers) - &lt;em&gt;Maison pour la Science Alpes-Dauphiné&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2023/2024&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;L&amp;rsquo;apprentissage continu de représentations visuelles - master level - &lt;em&gt;ENSIMAG, Grenoble&lt;/em&gt; [&lt;a href=&#34;https://project.inria.fr/bigvisdata/&#34;&gt;webpage&lt;/a&gt;]&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Dec 2023&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Lifelong visual representation learning  - &lt;em&gt;Master AI 4 One Health, UGA, Grenoble&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Sep 2023&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Keynote speaker at the &lt;em&gt;VISMAC International Summer School on Machine Vision&lt;/em&gt; (&lt;a href=&#34;https://vismac23.github.io&#34;&gt;VISMAC 2023&lt;/a&gt;)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2023&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Découvrir l’IA par le jeu (&lt;em&gt;Discover AI with games&lt;/em&gt;, a 2 day training session for high school teachers) - &lt;em&gt;Maison pour la Science Alpes-Dauphiné&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2022/2023&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;L&amp;rsquo;apprentissage continu de représentations visuelles - master level -  &lt;em&gt;ENSIMAG, Grenoble&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Dec 2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Lifelong visual representation learning  - &lt;em&gt;Master AI 4 One Health, UGA, Grenoble&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Découvrir l’IA par le jeu (&lt;em&gt;Discover AI with games&lt;/em&gt;, a 2 day training session for high school teachers) - &lt;em&gt;Maison pour la Science Alpes-Dauphiné&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Jan 2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Guest lecture on Visual Search  - &lt;em&gt;Posts and Telecommunications Institute of Technology (PTIT), Vietnam&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2021/2022&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Comprendre des données visuelles à grande échelle - master level - &lt;em&gt;ENSIMAG, Grenoble&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Nov 2021&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Lifelong visual representation learning  - &lt;em&gt;Master AI 4 One Health, UGA, Grenoble&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;May 2021&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Guest lecture on Visual Search  - &lt;em&gt;Hanoi University of Science and Technology (HUST), Vietnam&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2020/2021&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Comprendre des données visuelles à grande échelle - &lt;em&gt;ENSIMAG, Grenoble&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2020&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Découvrir l’IA par le jeu (&lt;em&gt;Discover AI with games&lt;/em&gt;, a 2 day training session for high school teachers) - &lt;em&gt;Maison pour la Science Alpes-Dauphiné&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Nov 2019&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Lecture on Visual Search as part of the &amp;lsquo;Deep Learning&amp;rsquo; course for the MVA master - &lt;em&gt;ENS Paris&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2019/2020&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Comprendre des données visuelles à grande échelle - master level - &lt;em&gt;ENSIMAG, Grenoble&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;Feb 2019&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Lecture on Visual Search as part of the &amp;lsquo;Deep Learning in Practice&amp;rsquo; course for the MVA master - &lt;em&gt;ENS Paris&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;2018/2019&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Comprendre des données visuelles à grande échelle - master level - &lt;em&gt;ENSIMAG, Grenoble&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;strong&gt;July 2018&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Lecture on Visual Search at the &lt;em&gt;PRAIRIE Artificial Intelligence summer school&lt;/em&gt; (&lt;a href=&#34;https://project.inria.fr/paiss/program-2018/&#34;&gt;PAISS 2018&lt;/a&gt;)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: right&#34;&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Older&lt;/strong&gt;: see CV&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;</description>
    </item>
    <item>
      <title>News</title>
      <link>/older_news/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/older_news/</guid>
      <description>&lt;p&gt;2022&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Co-organizing the &lt;a href=&#34;https://sslwin.org/&#34;&gt;Self Supervised Learning: What is Next?&lt;/a&gt; workshop at &lt;a href=&#34;https://eccv2022.ecva.net/&#34;&gt;&lt;strong&gt;ECCV22&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Paper accepted at &lt;a href=&#34;https://3dvconf.github.io/2022/&#34;&gt;&lt;strong&gt;3DV22&lt;/strong&gt;&lt;/a&gt;: &lt;em&gt;Neural Feature Fusion Fields: 3D Distillation of Self-Supervised 2D Image Representations&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2209.03494&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://www.robots.ox.ac.uk/~vadim/n3f/&#34;&gt;project page&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/qAIpStmMHjY&#34;&gt;video&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;li&gt;Keynote speaker at the joint &lt;a href=&#34;https://caprfiap2022.sciencesconf.org/&#34;&gt;&lt;strong&gt;CAp-RFIAP 2022&lt;/strong&gt;&lt;/a&gt; conference&lt;/li&gt;&#xA;&lt;li&gt;Paper accepted at &lt;a href=&#34;https://eccv2022.ecva.net/&#34;&gt;&lt;strong&gt;ECCV22&lt;/strong&gt;&lt;/a&gt;: &lt;em&gt;Granularity-aware Adaptation for Image Retrieval over Multiple Retrieval Tasks&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2210.02254&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;li&gt;Manuscript accepted to the &lt;a href=&#34;https://www.jmlr.org/tmlr/&#34;&gt;&lt;strong&gt;TMLR&lt;/strong&gt;&lt;/a&gt; journal: &lt;em&gt;TLDR: Twin Learning for Dimensionality Reduction&lt;/em&gt; [&lt;a href=&#34;https://openreview.net/forum?id=86fhqdBUbx&#34;&gt;paper&lt;/a&gt;,&lt;a href=&#34;https://github.com/naver/tldr&#34;&gt;code&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;li&gt;Invited talk at the &lt;a href=&#34;https://asu-apg.github.io/odrum/&#34;&gt;&lt;strong&gt;O-DRUM&lt;/strong&gt;&lt;/a&gt; workshop at &lt;a href=&#34;https://cvpr2022.thecvf.com/&#34;&gt;&lt;strong&gt;CVPR22&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Paper accepted at &lt;a href=&#34;https://cvpr2022.thecvf.com/&#34;&gt;&lt;strong&gt;CVPR22&lt;/strong&gt;&lt;/a&gt;: &lt;em&gt;On the Road to Online Adaptation for Semantic Image Segmentation&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2203.16195&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/naver/oasis&#34;&gt;code and benchmark&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;li&gt;2 papers accepted at &lt;a href=&#34;https://iclr.cc/&#34;&gt;&lt;strong&gt;ICLR22&lt;/strong&gt;&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;em&gt;ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2203.08101&#34;&gt;paper&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Learning Super-Features for Image Retrieval&lt;/em&gt; [&lt;a href=&#34;https://arxiv.org/abs/2201.13182&#34;&gt;paper&lt;/a&gt;,&lt;a href=&#34;https://github.com/naver/FIRe&#34;&gt;code&lt;/a&gt;]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Co-organizing the &lt;a href=&#34;https://meta.wikimedia.org/wiki/Wiki-M3L&#34;&gt;&lt;strong&gt;Wiki-M3L&lt;/strong&gt;&lt;/a&gt; workshop at &lt;a href=&#34;https://iclr.cc/&#34;&gt;&lt;strong&gt;ICLR22&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Area Chair for &lt;a href=&#34;http://cvpr2022.thecvf.com/&#34;&gt;&lt;strong&gt;CVPR22&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;2021&lt;/p&gt;</description>
    </item>
    <item>
      <title>Older projects</title>
      <link>/older_projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/older_projects/</guid>
      <description>&lt;p&gt;Back to the &lt;a href=&#34;/index.html&#34;&gt;Main Page&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
